# SceneScape: Text-Driven Consistent Scene Generation
> Rafail Fridman, Amit Abecasis, Yoni Kasten, Tali Dekel<br>
> Weizmann Institute of Science, NVIDIA Research<br>
> [arXiv](https://arxiv.org/abs/2302.01133)<br>
> [Project page](https://scenescape.github.io)<br>

## Abstract
> We propose a method for *text-driven perpetual view generation* â€“ synthesizing long videos of arbitrary scenes solely from an input text describing the scene and camera poses. We introduce a novel framework that generates such videos in an online fashion by combining the generative power of a pre-trained text-to-image model with the geometric priors learned by a pre-trained monocular depth prediction model. To achieve 3D consistency, i.e., generating videos that depict geometrically-plausible scenes, we deploy an online test-time training to encourage the predicted depth map of the current frame to be geometrically consistent with the synthesized scene; the depth maps are used to construct a *unified* mesh representation of the scene, which is updated throughout the generation and is used for rendering. In contrast to previous works, which are applicable only for limited domains (e.g., landscapes), our framework generates diverse scenes, such as walkthroughs in spaceships, caves, or ice castles.

## Introduction
- Previous work on text-to-image synthesis has focused on generating a single image
- Generating 3D scenes is more challenging, as it requires generating a sequence of images
- Different images in a sequence must be consistent with each other and the text prompt
- Parallax and occlusion are important for generating realistic scenes

## Method
- Input: a text prompt $\bm{P}$ describing the target scene and a camera trajectory $\{\bm{C}_i\}_{i=1}^T$, where $\bm{C}_i$ is the camera pose for frame $i$
- Combines two models: a text-to-image diffusion model and a pre-trained monocular depth prediction model
  - Diffusion model synthesizes new content as the camera moves
  - Depth prediction model estimates geometry of new content
  - Models are unified in a single 3D representation for geometric consistency
- Depth model and inpainting model do not produce consistent predictions across time, so both are fine-tuned at inference time
### Initialization and Scene Representation
- Text-to-image inpainting model $\bm{\phi}$ takes a text prompt $\bm{P}$, a mask $\bm{M}$, and a masked conditioning image $\bm{I}_t^M$:
    $$\bm{I}_t = \bm{\phi}(\bm{M}, \bm{I}_t^M, \bm{P})$$
- First frame $\bm{I}_0$ is generated using a prompt $\bm{P}$ and without masking ($\bm{M}$ all ones)
  - Then fed to depth prediction model $g$, which estimates the depth map $\bm{D}_0$ for $\bm{I}_0$: $\bm{D}_0 = g(\bm{I}_0)$
- Unified triangle mesh $\mathcal{M} = (\bm{V}, \bm{F}, \bm{E})$, where $\bm{V}$ is a set of vertices, $\bm{F}$ is a set of faces, and $\bm{E}$ is a set of edges
  - $\bm{V}$ is a set of 3D locations and color
  - $\mathcal{M}$ is initialized by unprojecting $(\bm{I}_0, \bm{D}_0)$
- Next masked frame $\bm{I}_{t+1}$ and masked depth map $\bm{D}_{t+1}$ are generated by projecting $\mathcal{M}$ to the next camera pose $\bm{C}_{t+1}$:
- The masked scene is inpainted: $\bm{I}_{t+1} = \bm{\phi}(\bm{M}, \bm{I}_{t+1}^M, \bm{P})$
- The masked depth is also optimized: $\bm{D}_{t+1} = g(\bm{I}_{t+1})$
- Finally, the unified mesh is updated: $\mathcal{M}_{t+1} = \mathcal{M}_t \cup \textit{UnProj}(\mathcal{M}_t, \bm{I}_{t+1}, \bm{D}_{t+1}), \bm{C}_{t+1}$
  - Existing parts of the mesh are not updated